{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exploring_multiLingual_sentence_transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhuXbq8qCOZ8Pnj/2gVzHn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCud8rAcB4lE"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "ted = datasets.load_dataset('ted_multi', split='train')\n",
        "ted\n",
        "\n",
        "\n",
        "\n",
        "ted[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the index\n",
        "idx = ted[0]['translations']['language'].index('en')\n",
        "idx"
      ],
      "metadata": {
        "id": "zY4K6NsdB6uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# use the index to get the corresponding translation\n",
        "source = ted[0]['translations']['translation'][idx]\n",
        "source\n",
        "\n"
      ],
      "metadata": {
        "id": "5ehONyl7CDO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use that info to create all (source, translation) pairs\n",
        "pairs = []\n",
        "for i, translation in enumerate(ted[0]['translations']['translation']):\n",
        "    # we don't want to use the source language (English) as a translation\n",
        "    if i != idx:\n",
        "        pairs.append((source, translation))\n",
        "\n",
        "# let's see what we have\n",
        "pairs[0]\n"
      ],
      "metadata": {
        "id": "M1fhZacoCF8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample\n",
        "from tqdm.auto import tqdm  # so we see progress bar\n",
        "\n",
        "# initialize list of languages to keep\n",
        "lang_list = ['it', 'es', 'ar', 'fr', 'de']\n",
        "# create dict to store our pairs\n",
        "train_samples = {f'en-{lang}': [] for lang in lang_list}\n",
        "\n",
        "# now build our training samples list\n",
        "for row in tqdm(ted):\n",
        "    # get source (English)\n",
        "    idx = row['translations']['language'].index('en')\n",
        "    source = row['translations']['translation'][idx].strip()\n",
        "    # loop through translations\n",
        "    for i, lang in enumerate(row['translations']['language']):\n",
        "        # check if lang is in lang list\n",
        "        if lang in lang_list:\n",
        "            translation = row['translations']['translation'][i].strip()\n",
        "            train_samples[f'en-{lang}'].append(\n",
        "                source+'\\t'+translation\n",
        "            )"
      ],
      "metadata": {
        "id": "uNHAiGLpCIgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# how many pairs for each language?\n",
        "for lang_pair in train_samples.keys():\n",
        "    print(f'{lang_pair}: {len(train_samples[lang_pair])}')\n",
        "\n"
      ],
      "metadata": {
        "id": "ueLYV2YtC7gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "source+'\\t'+translation\n",
        "\n"
      ],
      "metadata": {
        "id": "7M-KSP0ODAYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import gzip\n",
        "\n",
        "if not os.path.exists('./data'):\n",
        "    os.mkdir('./data')\n",
        "\n",
        "# save to file, sentence transformers reader will expect tsv.gz file\n",
        "for lang_pair in train_samples.keys():\n",
        "    with gzip.open(f'./data/ted-train-{lang_pair}.tsv.gz', 'wt', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(train_samples[lang_pair]))\n",
        "\n"
      ],
      "metadata": {
        "id": "Z_iqbSZTDFx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "rrG1bsQnDJGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sentences = [\n",
        "    'we will include several languages',\n",
        "    '一些中文单词',\n",
        "    'το ελληνικό αλφάβητο είναι πολύ ωραίο',\n",
        "    'ჩვენ გვაქვს ქართული'\n",
        "]\n",
        "\n",
        "for text in sentences:\n",
        "    print(bert_tokenizer.tokenize(text))\n",
        "\n"
      ],
      "metadata": {
        "id": "jmYoVVD3DPES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n"
      ],
      "metadata": {
        "id": "OkTZZHuGDTEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for text in sentences:\n",
        "    print(xlmr_tokenizer.tokenize(text))\n",
        "\n"
      ],
      "metadata": {
        "id": "-OGD3Y1gDZli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import models\n",
        "\n",
        "xlmr = models.Transformer('xlm-roberta-base')\n",
        "pooler = models.Pooling(\n",
        "    xlmr.get_word_embedding_dimension(),\n",
        "    pooling_mode_mean_tokens=True\n",
        ")\n",
        "\n",
        "student = SentenceTransformer(modules=[xlmr, pooler])\n",
        "student"
      ],
      "metadata": {
        "id": "6a62nCAgDc8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "teacher = SentenceTransformer('all-mpnet-base-v2')\n",
        "teacher\n",
        "\n"
      ],
      "metadata": {
        "id": "pUdmFTu4DgdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "teacher = SentenceTransformer('paraphrase-distilroberta-base-v2')\n",
        "teacher\n",
        "\n"
      ],
      "metadata": {
        "id": "tpavI1T4Dnf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import ParallelSentencesDataset\n",
        "data = ParallelSentencesDataset(student_model=student, teacher_model=teacher, batch_size=32, use_embedding_cache=True)"
      ],
      "metadata": {
        "id": "F9J9Qcr5FZ9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sentences_per_language = 500000\n",
        "train_max_sentence_length = 250 # max num of characters per sentence\n",
        "\n",
        "train_files = [f for f in os.listdir('./data') if 'train' in f]\n",
        "for f in train_files:\n",
        "    print(f)\n",
        "    data.load_data('./data/'+f, max_sentences=max_sentences_per_language, max_sentence_length=train_max_sentence_length)"
      ],
      "metadata": {
        "id": "lGB4PnfTGNRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader = DataLoader(data, shuffle=True, batch_size=32)\n",
        "\n"
      ],
      "metadata": {
        "id": "kFUkFOzaGz0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "loss = losses.MSELoss(model=student)"
      ],
      "metadata": {
        "id": "3BErYIDqG7su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sentence_transformers import evaluation\n",
        "import numpy as np\n",
        "\n",
        "epochs = 1\n",
        "warmup_steps = int(len(loader) * epochs * 0.1)\n",
        "\n",
        "student.fit(\n",
        "    train_objectives=[(loader, loss)],\n",
        "    epochs=epochs,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path='./xlmr-ted',\n",
        "    optimizer_params={'lr': 2e-5, 'eps': 1e-6, 'correct_bias': False},\n",
        "    save_best_model=True,\n",
        "    show_progress_bar=False\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "dZnzIZIPG-g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "en = datasets.load_dataset('stsb_multi_mt', 'en', split='test')\n",
        "en"
      ],
      "metadata": {
        "id": "leh1U-JjHEas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "it = datasets.load_dataset('stsb_multi_mt', 'it', split='test')\n",
        "it"
      ],
      "metadata": {
        "id": "yCaEd0dtHcDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en[0]\n",
        "\n",
        "it[0]"
      ],
      "metadata": {
        "id": "XNnwh7RrHe1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en = en.map(lambda x: {'similarity_score': x['similarity_score'] / 5.0})\n",
        "it = it.map(lambda x: {'similarity_score': x['similarity_score'] / 5.0})\n",
        "\n",
        "en[0]"
      ],
      "metadata": {
        "id": "otoT7lw_HkbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample\n",
        "\n",
        "en_samples = []\n",
        "it_samples = []\n",
        "en_it_samples = []\n",
        "\n",
        "for i in range(len(en)):\n",
        "    en_samples.append(InputExample(\n",
        "        texts=[en[i]['sentence1'], en[i]['sentence2']],\n",
        "        label=en[i]['similarity_score']\n",
        "    ))\n",
        "    it_samples.append(InputExample(\n",
        "        texts=[it[i]['sentence1'], it[i]['sentence2']],\n",
        "        label=it[i]['similarity_score']\n",
        "    ))\n",
        "    en_it_samples.append(InputExample(\n",
        "        texts=[en[i]['sentence1'], it[i]['sentence2']],\n",
        "        label=en[i]['similarity_score']\n",
        "    ))\n"
      ],
      "metadata": {
        "id": "ldXL1lXUHp4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "en_eval = EmbeddingSimilarityEvaluator.from_input_examples(\n",
        "    en_samples, write_csv=False\n",
        ")\n",
        "it_eval = EmbeddingSimilarityEvaluator.from_input_examples(\n",
        "    it_samples, write_csv=False\n",
        ")\n",
        "en_it_eval = EmbeddingSimilarityEvaluator.from_input_examples(\n",
        "    en_it_samples, write_csv=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "OtpclgYMHvkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('./xlmr-ted')\n",
        "\n",
        "en_eval(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "szpW1L0mH17L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "it_eval(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "x4MYV8a1H5wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "en_it_eval(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "J5TOB0zKIdqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sentence_transformers import models\n",
        "\n",
        "xlmr = models.Transformer('xlm-roberta-base')\n",
        "pooler = models.Pooling(\n",
        "    xlmr.get_word_embedding_dimension(),\n",
        "    pooling_mode_mean_tokens=True\n",
        ")\n",
        "\n",
        "student = SentenceTransformer(modules=[xlmr, pooler])\n",
        "\n"
      ],
      "metadata": {
        "id": "yujWuTEmIsy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "\n",
        "\n",
        "embed = model.encode\n"
      ],
      "metadata": {
        "id": "dflWjaGjIwZa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}