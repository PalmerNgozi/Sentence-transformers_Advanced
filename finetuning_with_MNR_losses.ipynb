{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetuning_with_MNR losses.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMnjpkGIFidgEZ7JuRytCJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztzX9YtSFHEa"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "snli = datasets.load_dataset('snli', split='train')\n",
        "\n",
        "snli\n",
        "\n",
        "\n",
        "mnli = datasets.load_dataset('glue', 'mnli', split='train')\n",
        "\n",
        "mnli = mnli.remove_columns(['idx'])\n",
        "\n",
        "\n",
        "snli = snli.cast(mnli.features)\n",
        "\n",
        "dataset = datasets.concatenate_datasets([snli, mnli])\n",
        "\n",
        "del snli, mnli"
      ],
      "metadata": {
        "id": "NDrPOU0sFfjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(f\"before: {len(dataset)} rows\")\n",
        "dataset = dataset.filter(\n",
        "    lambda x: True if x['label'] == 0 else False\n",
        ")\n",
        "print(f\"after: {len(dataset)} rows\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tJ1L-Kw1Fkvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        
        "#here we install the necessary packages. feel free to comment this part if you already have it installed\n",
        "!pip install sentence-transformers\n",
        "\n",
        "from sentence_transformers import InputExample\n",
        "from tqdm.auto import tqdm  # so we see progress bar\n",
        "\n",
        "train_samples = []\n",
        "for row in tqdm(dataset):\n",
        "    train_samples.append(InputExample(\n",
        "        texts=[row['premise'], row['hypothesis']]\n",
        "    ))\n",
        "\n"
      ],
      "metadata": {
        "id": "f1bz_pNjFrd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import datasets\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "loader = datasets.NoDuplicatesDataLoader(\n",
        "    train_samples, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "BQVAPJNQFyV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import models, SentenceTransformer\n",
        "\n",
        "bert = models.Transformer('bert-base-uncased')\n",
        "pooler = models.Pooling(\n",
        "    bert.get_word_embedding_dimension(),\n",
        "    pooling_mode_mean_tokens=True\n",
        ")\n",
        "\n",
        "model = SentenceTransformer(modules=[bert, pooler])\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "Or6U38uaF1GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "loss = losses.MultipleNegativesRankingLoss(model)"
      ],
      "metadata": {
        "id": "Nsgd2yO5F5rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "epochs = 1\n",
        "warmup_steps = int(len(loader) * epochs * 0.1)\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(loader, loss)],\n",
        "    epochs=epochs,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path='./sbert_test_mnr2',\n",
        "    show_progress_bar=False\n",
        ") \n"
      ],
      "metadata": {
        "id": "OYolX0lZGAFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " sentences = [\n",
        "    \"the fifty mannequin heads floating in the pool kind of freaked them out\",\n",
        "    \"she swore she just saw her sushi move\",\n",
        "    \"he embraced his new life as an eggplant\",\n",
        "    \"my dentist tells me that chewing bricks is very bad for your teeth\",\n",
        "    \"the dental specialist recommended an immediate stop to flossing with construction materials\",\n",
        "    \"i used to practice weaving with spaghetti three hours a day\",\n",
        "    \"the white water rafting trip was suddenly halted by the unexpected brick wall\",\n",
        "    \"the person would knit using noodles for a few hours daily\",\n",
        "    \"it was always dangerous to drive with him since he insisted the safety cones were a slalom course\",\n",
        "    \"the woman thinks she saw her raw fish and rice change position\"\n",
        "]"
      ],
      "metadata": {
        "id": "JzjptTQ8GcL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedds = model.encode(sentneces)"
      ],
      "metadata": {
        "id": "mrGsuWwDKlRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentece_transformers import cos_sim as cs\n",
        "\n",
        "def simX(model):\n",
        "  sim = n.zeros((len(sentences), len(sentences)))\n",
        "  for i in range(len(sentences)):\n",
        "    sim[i:,i] = cs(embeds[i], embeds[i:])\n",
        "\n",
        "  return sim"
      ],
      "metadata": {
        "id": "iO0E3gwWK_Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.py as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sim = simX(model)\n",
        "sns.heatmap(sim, annot= True)"
      ],
      "metadata": {
        "id": "CfQqw_ZYLohG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_F5G06UCMKrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
