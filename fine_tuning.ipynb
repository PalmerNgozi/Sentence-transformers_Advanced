{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nJxN9lGmAwp"
      },
      "outputs": [],
      "source": [
        "#We are using the datasets library from HUgging face. Feel free to skip this block if you already have it installed\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ELQ-Q-xmE-W"
      },
      "outputs": [],
      "source": [
        "#In order to download and merge the two corpora that is Stanford natural language inference and multi-genre natural language inference corpora\n",
        "\n",
        "import datasets\n",
        "\n",
        "s_nli = datasets.load_dataset('snli', split = 'train')\n",
        "\n",
        "s_nli\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA1kLDSBm05F",
        "outputId": "d3c28891-346b-4977-a04f-a6582d6f59aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'premise': 'A person on a horse jumps over a broken down airplane.', 'hypothesis': 'A person is training his horse for a competition.', 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "print(s_nli[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR7cdQEUnUpY"
      },
      "outputs": [],
      "source": [
        "#loading the dataset in this format\n",
        "\n",
        "m_nli = datasets. load_dataset('glue', 'mnli', split = 'train')\n",
        "\n",
        "m_nli\n",
        "print(m_nli[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDKR--w6nnkb"
      },
      "outputs": [],
      "source": [
        "#the idx column is not useful for us as thhe stanford dataset do not have that column. and in order for us to merge the dataset, they mush have the same name and number of columns\n",
        "\n",
        "m_nli = m_nli.remove_columns(['idx'])\n",
        "s_nli = s_nli.cast(m_nli.features)\n",
        "dataset = datasets.concatenate_datasets([s_nli, m_nli])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahJmhFxDobH2"
      },
      "outputs": [],
      "source": [
        "print(len(dataset))\n",
        "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
        "dataset = dataset.filter(\n",
        "    lambda x: 0 if x['label'] == -1 else 1\n",
        ")\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUGg54S8tODd"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers # uncomment this if you need to install sentence transformers\n",
        "\n",
        "from sentence_transformers import InputExample \n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "training_examples = []\n",
        "for row in tqdm(dataset): \n",
        "  training_examples.append(InputExample(\n",
        "      texts=[row['premise'], row['hypothesis']],\n",
        "      label = row['label']\n",
        "  ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQJDz55cq4p-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader as dl\n",
        "\n",
        "batch_size = 16\n",
        "loader = dl(training_examples, batch_size= batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaCLhYDu9eI"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import models, SentenceTransformer\n",
        "\n",
        "bert_model = models.Transformer('bert-base-uncased')\n",
        "\n",
        "pooler = models.Pooling(\n",
        "    bert_model.get_word_embedding_dimension(),\n",
        "    pooling_mode_mean_tokens=True\n",
        ")\n",
        "model = SentenceTransformer(modules= [bert_model,pooler])\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgydx9NyyZ_T"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "loss = losses.SoftmaxLoss(\n",
        "    model = model,\n",
        "    sentence_embedding_dimension= model.get_sentence_embedding_dimension(),\n",
        "     num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLQflU876yCJ"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "warm_steps = int(len(loader) * epochs *0.1)\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(loader,loss)], \n",
        "    epochs = epochs,\n",
        "    warmup_steps=warm_steps,\n",
        "    output_path='./sbert_test_b',\n",
        "    show_progress_bar=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56kPpnH-lWSM"
      },
      "outputs": [],
      "source": [
        " sentences = [\n",
        "    \"the fifty mannequin heads floating in the pool kind of freaked them out\",\n",
        "    \"she swore she just saw her sushi move\",\n",
        "    \"he embraced his new life as an eggplant\",\n",
        "    \"my dentist tells me that chewing bricks is very bad for your teeth\",\n",
        "    \"the dental specialist recommended an immediate stop to flossing with construction materials\",\n",
        "    \"i used to practice weaving with spaghetti three hours a day\",\n",
        "    \"the white water rafting trip was suddenly halted by the unexpected brick wall\",\n",
        "    \"the person would knit using noodles for a few hours daily\",\n",
        "    \"it was always dangerous to drive with him since he insisted the safety cones were a slalom course\",\n",
        "    \"the woman thinks she saw her raw fish and rice change position\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZxNG-1USnJQ"
      },
      "outputs": [],
      "source": [
        "embeds = model.encode(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Set_okbgSxwr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sentece_transformers import cos_sim as cs\n",
        "\n",
        "def simX(model):\n",
        "  sim = n.zeros((len(sentences), len(sentences)))\n",
        "  for i in range(len(sentences)):\n",
        "    sim[i:,i] = cs(embeds[i], embeds[i:])\n",
        "\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmOjbZ5NT63u"
      },
      "outputs": [],
      "source": [
        "#OR\n",
        "\n",
        "#sim = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "#for i in range(len(sentences)):\n",
        " #   sim[i:,i] = cs(embeddings[i], embeddings[i:])\n",
        "\n",
        "#sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Cd6CjBDSyhY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sim = simX(model)\n",
        "\n",
        "sns.heatmap(sim, annot=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOhNkYgXBZZiYpRWH7Wjunl",
      "collapsed_sections": [],
      "name": "fine_tuning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
